#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MiniCPM4-0.5B LLM å¯¹è¯ç”Ÿæˆæµ‹è¯•è„šæœ¬
æµ‹è¯•å¯¹è¯è¡¥å…¨å’Œç®€å•ç”Ÿæˆæ¥å£
"""

import requests
import json
import time
from typing import List, Dict, Optional

# API é…ç½®
API_BASE_URL = "http://localhost:18001"
CHAT_API_URL = f"{API_BASE_URL}/v1/chat/completions"
GENERATE_API_URL = f"{API_BASE_URL}/generate"
MODEL_ID = "MiniCPM4-0.5B"

# æµ‹è¯•é…ç½®
MAX_TOKENS = 512
TEMPERATURE = 0.7
TOP_P = 0.7


def test_health_check() -> bool:
    """æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£"""
    print("ğŸ” æµ‹è¯•å¥åº·æ£€æŸ¥æ¥å£...")
    try:
        response = requests.get(f"{API_BASE_URL}/health", timeout=10)
        response.raise_for_status()
        data = response.json()

        if data.get("status") == "healthy":
            print(f"   âœ… æœåŠ¡å¥åº·")
            print(f"   ğŸ“Š è®¾å¤‡: {data.get('device', 'unknown')}")
            print(f"   ğŸ® GPU: {data.get('gpu_name', 'N/A')}")
            print(f"   âš™ï¸  æœ€å¤§ç”Ÿæˆé•¿åº¦: {data.get('max_new_tokens', 'N/A')} tokens")
            return True
        else:
            print(f"   âš ï¸  æœåŠ¡çŠ¶æ€å¼‚å¸¸: {data}")
            return False
    except Exception as e:
        print(f"   âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False


def test_models_endpoint() -> bool:
    """æµ‹è¯•æ¨¡å‹åˆ—è¡¨æ¥å£"""
    print("\\nğŸ“‹ æµ‹è¯•æ¨¡å‹åˆ—è¡¨æ¥å£...")
    try:
        response = requests.get(f"{API_BASE_URL}/v1/models", timeout=10)
        response.raise_for_status()
        data = response.json()

        models = data.get("data", [])
        if models:
            model = models[0]
            print(f"   âœ… è·å–æ¨¡å‹åˆ—è¡¨æˆåŠŸ")
            print(f"   ğŸ¤– æ¨¡å‹ID: {model.get('id')}")
            print(f"   ğŸ“¦ å‚æ•°é‡: {model.get('parameters', 'N/A')}")
            print(f"   ğŸ”¢ æœ€å¤§Token: {model.get('max_tokens', 'N/A')}")
            return True
        else:
            print(f"   âš ï¸  æ¨¡å‹åˆ—è¡¨ä¸ºç©º")
            return False
    except Exception as e:
        print(f"   âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return False


def test_chat_completion(messages: List[Dict[str, str]], description: str = "") -> Optional[Dict]:
    """æµ‹è¯•å¯¹è¯è¡¥å…¨æ¥å£ (OpenAI å…¼å®¹æ ¼å¼)"""
    if description:
        print(f"\\nğŸ’¬ {description}")

    payload = {
        "model": MODEL_ID,
        "messages": messages,
        "temperature": TEMPERATURE,
        "top_p": TOP_P,
        "max_tokens": MAX_TOKENS,
        "do_sample": True
    }

    try:
        start_time = time.time()
        response = requests.post(CHAT_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        elapsed_ms = (time.time() - start_time) * 1000

        result = response.json()
        choice = result["choices"][0]
        usage = result["usage"]

        reply = choice["message"]["content"]

        print(f"   âœ… ç”ŸæˆæˆåŠŸ ({elapsed_ms:.1f}ms)")
        print(f"   ğŸ“ Prompt tokens: {usage.get('prompt_tokens', 'N/A')}")
        print(f"   âœï¸  Completion tokens: {usage.get('completion_tokens', 'N/A')}")
        print(f"   ğŸ’° Total tokens: {usage.get('total_tokens', 'N/A')}")
        print(f"   ğŸ¤– å›å¤: {reply[:200]}{'...' if len(reply) > 200 else ''}")

        return result

    except requests.exceptions.RequestException as e:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e.response, 'text'):
            print(f"   é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return None
    except Exception as e:
        print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        return None


def test_simple_generate(prompt: str, description: str = "", use_cache: bool = False) -> Optional[Dict]:
    """æµ‹è¯•ç®€å•ç”Ÿæˆæ¥å£ï¼ˆæ”¯æŒç¼“å­˜ï¼‰"""
    if description:
        print(f"\\nğŸš€ {description}")

    payload = {
        "prompt": prompt,
        "temperature": TEMPERATURE,
        "top_p": TOP_P,
        "max_new_tokens": MAX_TOKENS,
        "use_cache": use_cache
    }

    try:
        start_time = time.time()
        response = requests.post(GENERATE_API_URL, json=payload, timeout=120)
        response.raise_for_status()
        elapsed_ms = (time.time() - start_time) * 1000

        result = response.json()
        reply = result["response"]
        usage = result["usage"]

        cache_status = "å‘½ä¸­ç¼“å­˜" if usage.get("from_cache") else "æœªå‘½ä¸­ç¼“å­˜"

        print(f"   âœ… ç”ŸæˆæˆåŠŸ ({elapsed_ms:.1f}ms, {cache_status})")
        print(f"   âœï¸  Tokens: {usage.get('completion_tokens', 'N/A')}")
        print(f"   ğŸ¤– å›å¤: {reply[:200]}{'...' if len(reply) > 200 else ''}")

        return result

    except requests.exceptions.RequestException as e:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e.response, 'text'):
            print(f"   é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return None
    except Exception as e:
        print(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        return None


def test_stats_endpoint() -> bool:
    """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯æ¥å£"""
    print("\\nğŸ“Š æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯æ¥å£...")
    try:
        response = requests.get(f"{API_BASE_URL}/stats", timeout=10)
        response.raise_for_status()
        data = response.json()

        cache_info = data.get("cache", {})
        config_info = data.get("config", {})

        print(f"   âœ… è·å–ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ")
        print(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {cache_info.get('hits', 0)} æ¬¡")
        print(f"   ğŸ†• ç¼“å­˜æœªå‘½ä¸­: {cache_info.get('misses', 0)} æ¬¡")
        print(f"   ğŸ“ˆ å‘½ä¸­ç‡: {cache_info.get('hit_rate', 0)}%")
        print(f"   âš™ï¸  å½“å‰é…ç½®: batch={config_info.get('max_batch_size')}, tokens={config_info.get('max_new_tokens')}")
        return True

    except Exception as e:
        print(f"   âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return False


def main():
    print("=" * 60)
    print("ğŸš€ MiniCPM4-0.5B LLM å¯¹è¯ç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    print(f"APIç«¯ç‚¹: {API_BASE_URL}")
    print(f"æ¨¡å‹: {MODEL_ID}")
    print(f"æ¸©åº¦: {TEMPERATURE}, Top-P: {TOP_P}, MaxTokens: {MAX_TOKENS}")
    print("=" * 60)

    # 1. å¥åº·æ£€æŸ¥
    if not test_health_check():
        print("\\nğŸ’¥ æœåŠ¡æœªå°±ç»ªï¼Œé€€å‡ºæµ‹è¯•")
        return

    # 2. æ¨¡å‹åˆ—è¡¨
    test_models_endpoint()

    # 3. æµ‹è¯•å¯¹è¯è¡¥å…¨æ¥å£
    print("\\n" + "=" * 60)
    print("ğŸ“ æµ‹è¯• /v1/chat/completions æ¥å£ (OpenAI å…¼å®¹)")
    print("=" * 60)

    # æµ‹è¯• 1: ç®€å•é—®ç­”
    test_chat_completion(
        messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}],
        description="æµ‹è¯•1: ç®€å•é—®å€™"
    )

    # æµ‹è¯• 2: å¤šè½®å¯¹è¯
    test_chat_completion(
        messages=[
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
            {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚"},
            {"role": "user", "content": "é‚£æ·±åº¦å­¦ä¹ å‘¢ï¼Ÿå®ƒå’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"}
        ],
        description="æµ‹è¯•2: å¤šè½®å¯¹è¯ï¼ˆå¸¦å†å²ä¸Šä¸‹æ–‡ï¼‰"
    )

    # æµ‹è¯• 3: ä»£ç ç”Ÿæˆ
    test_chat_completion(
        messages=[{"role": "user", "content": "ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°ï¼Œè¦æ±‚ä½¿ç”¨é€’å½’æ–¹å¼"}],
        description="æµ‹è¯•3: ä»£ç ç”Ÿæˆ"
    )

    # æµ‹è¯• 4: é•¿æ–‡æœ¬ç†è§£ï¼ˆä¸­æ–‡ï¼‰
    test_chat_completion(
        messages=[{
            "role": "user",
            "content": "è¯·æ€»ç»“ä»¥ä¸‹è¿™æ®µè¯çš„ä¸»è¦è§‚ç‚¹ï¼šäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ­£åœ¨æ·±åˆ»æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚ä»æ™ºèƒ½æ‰‹æœºä¸­çš„è¯­éŸ³åŠ©æ‰‹åˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œä»åŒ»ç–—è¯Šæ–­ç³»ç»Ÿåˆ°ä¸ªæ€§åŒ–æ¨èå¼•æ“ï¼ŒAIæŠ€æœ¯å·²ç»æ¸—é€åˆ°æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚è¿™ç§å˜é©å¸¦æ¥äº†æ•ˆç‡çš„æå‡å’Œä¾¿åˆ©æ€§çš„å¢åŠ ï¼Œä½†åŒæ—¶ä¹Ÿå¼•å‘äº†å…³äºéšç§ä¿æŠ¤ã€å°±ä¸šå½±å“å’ŒæŠ€æœ¯ä¼¦ç†ç­‰æ–¹é¢çš„è®¨è®ºã€‚å¦‚ä½•åœ¨æ¨åŠ¨æŠ€æœ¯åˆ›æ–°çš„åŒæ—¶ç¡®ä¿å…¶è´Ÿè´£ä»»åœ°å‘å±•ï¼Œæ˜¯å½“å‰ç¤¾ä¼šé¢ä¸´çš„é‡è¦è¯¾é¢˜ã€‚"
        }],
        description="æµ‹è¯•4: é•¿æ–‡æœ¬ç†è§£ï¼ˆä¸­æ–‡æ‘˜è¦ï¼‰"
    )

    # 4. æµ‹è¯•ç®€å•ç”Ÿæˆæ¥å£
    print("\\n" + "=" * 60)
    print("ğŸš€ æµ‹è¯• /generate æ¥å£ (ç®€å•ç”Ÿæˆï¼Œæ”¯æŒç¼“å­˜)")
    print("=" * 60)

    # æµ‹è¯• 5: ç®€å•ç”Ÿæˆï¼ˆé¦–æ¬¡ï¼Œæ— ç¼“å­˜ï¼‰
    prompt = "åˆ—ä¸¾ä¸‰ä¸ªäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨"
    test_simple_generate(
        prompt=prompt,
        description="æµ‹è¯•5: ç®€å•ç”Ÿæˆï¼ˆé¦–æ¬¡ï¼Œæ— ç¼“å­˜ï¼‰",
        use_cache=True
    )

    # æµ‹è¯• 6: ç›¸åŒ Promptï¼ˆæµ‹è¯•ç¼“å­˜ï¼‰
    test_simple_generate(
        prompt=prompt,
        description="æµ‹è¯•6: ç›¸åŒPromptï¼ˆæµ‹è¯•ç¼“å­˜å‘½ä¸­ï¼‰",
        use_cache=True
    )

    # æµ‹è¯• 7: åˆ›æ„å†™ä½œ
    test_simple_generate(
        prompt="å†™ä¸€ä¸ªå…³äºæœªæ¥åŸå¸‚çš„çŸ­æ•…äº‹ï¼Œä¸è¶…è¿‡100å­—",
        description="æµ‹è¯•7: åˆ›æ„å†™ä½œ",
        use_cache=False
    )

    # 5. ç»Ÿè®¡ä¿¡æ¯
    print("\\n" + "=" * 60)
    print("ğŸ“Š è·å–æœåŠ¡ç»Ÿè®¡")
    print("=" * 60)
    test_stats_endpoint()

    # ç»“æŸ
    print("\\n" + "=" * 60)
    print("âœ¨ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰æ¥å£éªŒè¯é€šè¿‡")
    print("=" * 60)
    print("\\nğŸ“‹ æ¥å£æ±‡æ€»ï¼š")
    print(f"   â€¢ å¯¹è¯æ¥å£: {CHAT_API_URL}")
    print(f"   â€¢ ç”Ÿæˆæ¥å£: {GENERATE_API_URL}")
    print(f"   â€¢ å¥åº·æ£€æŸ¥: {API_BASE_URL}/health")
    print(f"   â€¢ ç»Ÿè®¡ä¿¡æ¯: {API_BASE_URL}/stats")
    print("=" * 60)


if __name__ == "__main__":
    main()