import os
import sys
import torch
import numpy as np
from fastapi import FastAPI, HTTPException, Request, status
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Union, Literal
from transformers import AutoTokenizer, AutoModel  # â¬…ï¸ æ›¿æ¢ modelscope
import time
import logging
from functools import lru_cache
import hashlib

# ==================== ç¯å¢ƒåˆå§‹åŒ– ====================
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
MODEL_CACHE_DIR = os.path.join(PROJECT_ROOT,"models")

os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
os.environ["HF_HOME"] = MODEL_CACHE_DIR  # â¬…ï¸ åªç”¨ HF_HOMEï¼Œç§»é™¤ MODELSCOPE_*

print(f"âœ… æ¨¡å‹ç¼“å­˜ç›®å½•: {MODEL_CACHE_DIR}")
print(f"âœ… ç›®å½•å­˜åœ¨: {os.path.exists(MODEL_CACHE_DIR)}")

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ==================== è®¾å¤‡æ£€æµ‹ ====================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ğŸš€ æ£€æµ‹åˆ°è®¾å¤‡: {DEVICE} | PyTorch: {torch.__version__}")
if DEVICE == "cuda":
    logger.info(
        f"   GPU: {torch.cuda.get_device_name(0)} | "
        f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB"
    )
    # GTX 1060 ä¼˜åŒ–è®¾ç½®
    torch.backends.cuda.matmul.allow_tf32 = False  # Pascal ä¸æ”¯æŒ TF32
    torch.backends.cudnn.benchmark = True

# ==================== æ¨¡å‹åŠ è½½ï¼ˆTransformers æ–¹å¼ï¼‰====================
MODEL_PATH = os.getenv("MODEL_PATH", os.path.join(MODEL_CACHE_DIR, "Qwen", "Qwen3-Embedding-0___6B"))

try:
    logger.info(f"â³ åŠ è½½æ¨¡å‹: {MODEL_PATH} åˆ° {DEVICE}...")
    start = time.time()

    # GTX 1060 6GB å¿…é¡»ç”¨åŠç²¾åº¦ï¼Œå¦åˆ™æ˜¾å­˜ä¸å¤Ÿ
    dtype = torch.float16 if DEVICE == "cuda" else torch.float32

    # åŠ è½½ tokenizer å’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        cache_dir=MODEL_CACHE_DIR
    )
    model = AutoModel.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=dtype,
        cache_dir=MODEL_CACHE_DIR,
        # device_map="auto"  # å¯é€‰ï¼šè‡ªåŠ¨åˆ†é…å±‚åˆ° GPU/CPU
    )
    model.to(DEVICE)
    model.eval()

    # # ç¼–è¯‘æ¨¡å‹åŠ é€Ÿï¼ˆPyTorch 2.0+ï¼Œå¯é€‰ï¼‰
    # if hasattr(torch, 'compile') and DEVICE == "cuda":
    #     try:
    #         model = torch.compile(model, mode="reduce-overhead")
    #         logger.info("âœ… æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å·²å¯ç”¨")
    #     except Exception as e:
    #         logger.warning(f"âš ï¸ æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")

    load_time = time.time() - start
    logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ ({load_time:.2f}s)")

    # é¢„çƒ­
    with torch.no_grad():
        dummy_input = tokenizer("warmup", return_tensors="pt").to(DEVICE)
        _ = model(**dummy_input)
    logger.info("ğŸ”¥ æ¨¡å‹é¢„çƒ­å®Œæˆ")

except Exception as e:
    logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
    raise


# ==================== Embedding å‡½æ•° ====================
def get_embeddings(texts: List[str]) -> np.ndarray:
    """
    æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥ï¼ˆMean Pooling + L2 å½’ä¸€åŒ–ï¼‰
    """
    # Tokenize
    inputs = tokenizer(
        texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=8192  # Qwen3 æ”¯æŒ 32Kï¼Œä½† 8K è¶³å¤Ÿä¸”æ›´å¿«
    )
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

    with torch.no_grad():
        # Forward
        outputs = model(**inputs)
        hidden_states = outputs.last_hidden_state  # [batch, seq_len, hidden_dim]

        # Mean Poolingï¼ˆè€ƒè™‘ paddingï¼‰
        attention_mask = inputs['attention_mask']  # [batch, seq_len]
        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()
        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)  # [batch, hidden_dim]
        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
        embeddings = sum_embeddings / sum_mask

        # L2 å½’ä¸€åŒ–ï¼ˆå¿…é¡»åœ¨ GPU ä¸Šåšï¼Œå‡å°‘æ•°æ®ä¼ è¾“ï¼‰
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

        # ç§»å› CPU å¹¶è½¬ numpy
        return embeddings.cpu().numpy()


# ==================== å·¥å…·å‡½æ•° ====================
def hash_text(text: str) -> str:
    return hashlib.md5(text.encode('utf-8')).hexdigest()


# ==================== ç¼“å­˜æœºåˆ¶ ====================
CACHE_SIZE = int(os.getenv("CACHE_SIZE", "1000"))


@lru_cache(maxsize=CACHE_SIZE)
def cached_encode(text_hash: str, text: str) -> tuple:
    """
    LRU ç¼“å­˜ç¼–ç ç»“æœï¼ˆè¿”å› tuple å› ä¸º lru_cache éœ€è¦å¯å“ˆå¸Œå¯¹è±¡ï¼‰
    """
    embedding = get_embeddings([text])[0]  # [hidden_dim]
    return tuple(embedding.tolist())


# ==================== Pydantic æ¨¡å‹ ====================
class EmbeddingRequest(BaseModel):
    input: Union[str, List[str]] = Field(
        ...,
        description="è¾“å…¥æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰",
        example="The quick brown fox jumps over the lazy dog"
    )
    model: str = Field("Qwen3-Embedding-0.6B", description="æ¨¡å‹æ ‡è¯†ç¬¦")
    encoding_format: Literal["float", "base64"] = Field("float", description="è¾“å‡ºæ ¼å¼")
    dimensions: Optional[int] = Field(None, ge=32, le=1024, description="è¾“å‡ºç»´åº¦ï¼ˆ32-1024ï¼‰")
    normalize: bool = Field(True, description="âš ï¸ å¿…é¡»ä¸º Trueï¼Qwen3 åµŒå…¥å¿…é¡»å½’ä¸€åŒ–")

    @validator('input')
    def validate_input(cls, v):
        MAX_BATCH_SIZE = 64
        MAX_TEXT_LENGTH = 32768

        if isinstance(v, list):
            if not v:
                raise ValueError("è¾“å…¥åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            if len(v) > MAX_BATCH_SIZE:
                raise ValueError(f"æ‰¹é‡å¤§å°ä¸èƒ½è¶…è¿‡ {MAX_BATCH_SIZE}")
            for i, text in enumerate(v):
                if len(text) > MAX_TEXT_LENGTH:
                    raise ValueError(f"ç¬¬ {i + 1} æ¡æ–‡æœ¬é•¿åº¦è¶…è¿‡ {MAX_TEXT_LENGTH} å­—ç¬¦")
        elif isinstance(v, str):
            if len(v) > MAX_TEXT_LENGTH:
                raise ValueError(f"æ–‡æœ¬é•¿åº¦ä¸èƒ½è¶…è¿‡ {MAX_TEXT_LENGTH} å­—ç¬¦")
        else:
            raise ValueError("è¾“å…¥å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")
        return v


class Usage(BaseModel):
    prompt_tokens: int
    total_tokens: int
    processing_time_ms: float


class EmbeddingObject(BaseModel):
    object: str = "embedding"
    index: int
    embedding: List[float]


class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[EmbeddingObject]
    model: str = "Qwen3-Embedding-0.6B"
    usage: Usage


# ==================== FastAPI åº”ç”¨ ====================
app = FastAPI(
    title="Qwen3-Embedding-0.6B API",
    description="""
    ğŸš€ é«˜æ€§èƒ½æœ¬åœ°åµŒå…¥æœåŠ¡ | Qwen3-Embedding-0.6B (GPU Accelerated)

    ## å…³é”®ç‰¹æ€§
    - âœ… **GPU åŠ é€Ÿ**ï¼ˆGTX 1060 6GB ä¼˜åŒ–ï¼‰
    - âœ… **å¼ºåˆ¶ L2 å½’ä¸€åŒ–**ï¼ˆç›¸ä¼¼åº¦è®¡ç®—å¿…éœ€ï¼‰
    - âœ… **OpenAI å…¼å®¹æ¥å£**ï¼ˆæ— ç¼é›†æˆ LangChainï¼‰
    - âœ… **æœ¬åœ°æ¨¡å‹ç¼“å­˜**ï¼ˆmodels/ ç›®å½•ï¼Œä¸å ç”¨ C ç›˜ï¼‰
    - âœ… **LRU ç¼“å­˜**ï¼ˆé‡å¤æ–‡æœ¬åŠ é€Ÿ 10-50 å€ï¼‰

    ## é‡è¦æç¤º
    âš ï¸ **æ‰€æœ‰è¾“å‡ºå·²è‡ªåŠ¨å½’ä¸€åŒ–**ï¼ˆL2 èŒƒæ•° = 1ï¼‰ï¼Œå¯ç›´æ¥ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ã€‚
    """,
    version="2.0.0",  # â¬…ï¸ ç‰ˆæœ¬å‡çº§ï¼ˆç§»é™¤ modelscopeï¼‰
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== API ç«¯ç‚¹ ====================
@app.post(
    "/v1/embeddings",
    response_model=EmbeddingResponse,
    status_code=status.HTTP_200_OK,
    summary="ç”Ÿæˆæ–‡æœ¬åµŒå…¥"
)
async def create_embedding(request: EmbeddingRequest):
    start_time = time.time()
    texts = [request.input] if isinstance(request.input, str) else request.input
    batch_size = len(texts)

    logger.info(f"ğŸ“¥ å¤„ç† {batch_size} æ¡æ–‡æœ¬ | ç»´åº¦: {request.dimensions or 1024}")

    try:
        # æ‰¹é‡ç¼–ç ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        embeddings_list = []
        total_tokens = 0

        for idx, text in enumerate(texts):
            text_hash = hash_text(text)
            total_tokens += len(text.split())  # ç®€å• token ä¼°ç®—

            # ä»ç¼“å­˜è·å–ï¼ˆè¿”å› tupleï¼Œè½¬å› listï¼‰
            emb_tuple = cached_encode(text_hash, text)
            embedding = np.array(emb_tuple)

            # ç»´åº¦è£å‰ªï¼ˆQwen3 æ”¯æŒè¿è¡Œæ—¶è£å‰ªï¼‰
            if request.dimensions and request.dimensions < embedding.shape[0]:
                embedding = embedding[:request.dimensions]

            embeddings_list.append(embedding)

        # åˆå¹¶ä¸ºçŸ©é˜µ
        embeddings = np.stack(embeddings_list)  # [batch, dim]

        # æ„é€ å“åº”
        data = [
            EmbeddingObject(index=i, embedding=emb.tolist())
            for i, emb in enumerate(embeddings)
        ]

        processing_time_ms = (time.time() - start_time) * 1000
        logger.info(
            f"âœ… å®Œæˆ {batch_size} æ¡ | "
            f"ç»´åº¦: {embeddings.shape[1]} | "
            f"è€—æ—¶: {processing_time_ms:.2f}ms | "
            f"é€Ÿåº¦: {batch_size / (processing_time_ms / 1000):.1f} æ¡/ç§’"
        )

        return EmbeddingResponse(
            data=data,
            model=request.model,
            usage=Usage(
                prompt_tokens=total_tokens,
                total_tokens=total_tokens,
                processing_time_ms=round(processing_time_ms, 2)
            )
        )

    except Exception as e:
        logger.error(f"âŒ åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}")


@app.get("/health")
async def health_check():
    gpu_info = {}
    if DEVICE == "cuda":
        gpu_info = {
            "gpu_name": torch.cuda.get_device_name(0),
            "gpu_memory_total_gb": round(torch.cuda.get_device_properties(0).total_memory / 1024 ** 3, 2),
            "gpu_memory_used_mb": round(torch.cuda.memory_allocated(0) / 1024 ** 2, 2),
            "gpu_memory_reserved_mb": round(torch.cuda.memory_reserved(0) / 1024 ** 2, 2),
        }

    return {
        "status": "healthy",
        "version": "2.0.0",
        "device": DEVICE,
        "model": MODEL_PATH,
        "model_cache_dir": MODEL_CACHE_DIR,
        "cuda_available": torch.cuda.is_available(),
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
        **gpu_info
    }


@app.get("/v1/models")
async def list_models():
    return {
        "data": [{
            "id": "Qwen3-Embedding-0.6B",
            "object": "model",
            "created": 1700000000,
            "owned_by": "Alibaba",
            "dimensions": 1024,
            "max_input_tokens": 32768
        }],
        "object": "list"
    }


@app.get("/stats")
async def get_stats():
    cache_info = cached_encode.cache_info()
    total = cache_info.hits + cache_info.misses
    return {
        "cache": {
            "hits": cache_info.hits,
            "misses": cache_info.misses,
            "maxsize": cache_info.maxsize,
            "currsize": cache_info.currsize,
            "hit_rate": round(cache_info.hits / total * 100, 2) if total > 0 else 0.0
        },
        "device": DEVICE,
        "model": MODEL_PATH,
        "model_loaded": model is not None
    }


# ==================== å¯åŠ¨ ====================
if __name__ == "__main__":
    import uvicorn

    # æ£€æŸ¥æ¨¡å‹ç¼“å­˜
    model_cache_path = os.path.join(MODEL_CACHE_DIR, "Qwen", "Qwen3-Embedding-0___6B")
    if not os.path.exists(model_cache_path):
        logger.warning(f"âš ï¸  æ¨¡å‹ç¼“å­˜ä¸å­˜åœ¨: {model_cache_path}")
        logger.warning("   é¦–æ¬¡å¯åŠ¨å°†è‡ªåŠ¨ä¸‹è½½ï¼ˆçº¦ 1.2GBï¼‰ï¼Œè¯·è€å¿ƒç­‰å¾…...")

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=18000,
        workers=1,  # GPU æ¨¡å¼å¿…é¡»å• worker
        log_level="info",
        reload=False
    )